{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "from collections import defaultdict\n",
    "from tokenizers import Tokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define the text to tokenize\n",
    "text = \"\"\"I must not fear. Fear is the mind-killer. Fear is the little-death that brings total obliteration. \n",
    "I will face my fear. I will permit it to pass over me and through me. \n",
    "And when it has gone past I will turn the inner eye to see its path. \n",
    "Where the fear has gone there will be nothing. Only I will remain.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Lowercase the text\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Implement the Word Tokenizer\n",
    "def word_tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Implement the Character Tokenizer\n",
    "def character_tokenizer(text):\n",
    "    return list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Implement the Sentence Tokenizer\n",
    "def sentence_tokenizer(text):\n",
    "    return re.split(r'[.!?]\\s*', text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Create a vocabulary function\n",
    "def create_vocabulary(tokens):\n",
    "    vocab = defaultdict(lambda: len(vocab))\n",
    "    [vocab[token] for token in tokens]\n",
    "    return dict(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Tokenize the text using each tokenizer\n",
    "word_tokens = word_tokenizer(text)\n",
    "char_tokens = character_tokenizer(text)\n",
    "sentence_tokens = sentence_tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Create vocabularies for each tokenizer\n",
    "word_vocab = create_vocabulary(word_tokens)\n",
    "char_vocab = create_vocabulary(char_tokens)\n",
    "sentence_vocab = create_vocabulary(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Convert tokens to indices\n",
    "word_indices = [word_vocab[token] for token in word_tokens]\n",
    "char_indices = [char_vocab[token] for token in char_tokens]\n",
    "sentence_indices = [sentence_vocab[token] for token in sentence_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Encode the sequence using a pretrained WordPiece tokenizer\n",
    "tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "wordpiece_indices = tokenizer.encode(text).ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenizer Output (Indices): [0, 1, 2, 3, 4, 5, 6, 7, 4, 5, 6, 8, 9, 10, 11, 12, 0, 13, 14, 15, 3, 0, 13, 16, 17, 18, 19, 20, 21, 22, 23, 24, 22, 25, 17, 26, 27, 28, 0, 13, 29, 6, 30, 31, 18, 32, 33, 34, 35, 6, 4, 26, 27, 36, 13, 37, 38, 39, 0, 13, 40]\n",
      "Character Tokenizer Output (Indices): [0, 1, 2, 3, 4, 5, 1, 6, 7, 5, 1, 8, 9, 10, 11, 12, 1, 8, 9, 10, 11, 1, 0, 4, 1, 5, 13, 9, 1, 2, 0, 6, 14, 15, 16, 0, 17, 17, 9, 11, 12, 1, 8, 9, 10, 11, 1, 0, 4, 1, 5, 13, 9, 1, 17, 0, 5, 5, 17, 9, 15, 14, 9, 10, 5, 13, 1, 5, 13, 10, 5, 1, 18, 11, 0, 6, 19, 4, 1, 5, 7, 5, 10, 17, 1, 7, 18, 17, 0, 5, 9, 11, 10, 5, 0, 7, 6, 12, 1, 20, 0, 1, 21, 0, 17, 17, 1, 8, 10, 22, 9, 1, 2, 23, 1, 8, 9, 10, 11, 12, 1, 0, 1, 21, 0, 17, 17, 1, 24, 9, 11, 2, 0, 5, 1, 0, 5, 1, 5, 7, 1, 24, 10, 4, 4, 1, 7, 25, 9, 11, 1, 2, 9, 1, 10, 6, 14, 1, 5, 13, 11, 7, 3, 19, 13, 1, 2, 9, 12, 1, 20, 10, 6, 14, 1, 21, 13, 9, 6, 1, 0, 5, 1, 13, 10, 4, 1, 19, 7, 6, 9, 1, 24, 10, 4, 5, 1, 0, 1, 21, 0, 17, 17, 1, 5, 3, 11, 6, 1, 5, 13, 9, 1, 0, 6, 6, 9, 11, 1, 9, 23, 9, 1, 5, 7, 1, 4, 9, 9, 1, 0, 5, 4, 1, 24, 10, 5, 13, 12, 1, 20, 21, 13, 9, 11, 9, 1, 5, 13, 9, 1, 8, 9, 10, 11, 1, 13, 10, 4, 1, 19, 7, 6, 9, 1, 5, 13, 9, 11, 9, 1, 21, 0, 17, 17, 1, 18, 9, 1, 6, 7, 5, 13, 0, 6, 19, 12, 1, 7, 6, 17, 23, 1, 0, 1, 21, 0, 17, 17, 1, 11, 9, 2, 10, 0, 6, 12]\n",
      "Sentence Tokenizer Output (Indices): [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "WordPiece Tokenizer Output (Indices): [101, 1045, 2442, 2025, 3571, 1012, 3571, 2003, 1996, 2568, 1011, 6359, 1012, 3571, 2003, 1996, 2210, 1011, 2331, 2008, 7545, 2561, 27885, 22779, 8156, 1012, 1045, 2097, 2227, 2026, 3571, 1012, 1045, 2097, 9146, 2009, 2000, 3413, 2058, 2033, 1998, 2083, 2033, 1012, 1998, 2043, 2009, 2038, 2908, 2627, 1045, 2097, 2735, 1996, 5110, 3239, 2000, 2156, 2049, 4130, 1012, 2073, 1996, 3571, 2038, 2908, 2045, 2097, 2022, 2498, 1012, 2069, 1045, 2097, 3961, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Print the results\n",
    "print(\"Word Tokenizer Output (Indices):\", word_indices)\n",
    "print(\"Character Tokenizer Output (Indices):\", char_indices)\n",
    "print(\"Sentence Tokenizer Output (Indices):\", sentence_indices)\n",
    "print(\"WordPiece Tokenizer Output (Indices):\", wordpiece_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
